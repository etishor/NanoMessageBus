Full unit and integration tests
XML code comments
Documentation and examples
Fluent builder
Support for multiple containers
ILMerge quirks

Multiple dispatch to handlers, e.g. IHandleMessages<object> gets everything...

Protocol Buffer serializer

Additional endpoint connectors, e.g. Azure, SQS, RabbitMQ, ActiveMQ, Qpid, etc.

Maximum # of failures & retry / error queue / (infrastructure message headers would contain exception)
 - or we could have another envelope around the error condition inside the PhysicalMessage
Serialization exception should move straight to error queue

Rename "DefaultImplementation" projects

IdempotentReceiver
 - when a message is received, we ensure that we've only received it once and that,
   when the UoW is complete, the message is considered received and then ignored
 - This would most likely occur in memory where we could track the last X,000 messages
   (or X minutes) received and de-duplicate on that...

SinglePhaseCommitReceiver/Sender
 - The message is received in a separate transaction from the work against the DB
 - This really works well for NoSQL implementations...
 - When the "DB" UoW is done, it commits and tracks the incoming message that was
   associated with the UoW.
 - Furthermore, it writes its results to an outbound "queue" table.
 - It pushes the message to publish onto an in-memory "queue" which another thread
   it monitoring and publishing from
 - This thread then does the publish and removes the item from the queue table.
 - The entire objective is to completely avoid 2PC so as to enable use of other
   storage engines that may not support 2PC outright.

ISplitMessagesIntoPackets and IReassembleMessagesFromPackets
 - allows smaller payloads to be split and transferred across the wire when the
   infrastructure has a highly restrictive message size, e.g. Azure & SQS.)

 - Packet: GroupId:Guid, Sequence/Index:int, Bytes:int, FinalPacket:bool
 - Split(Stream, MaxPacketSize):IEnumerable<Packet>
 -  Combine(Packet):Stream (null if !complete)

 - Expiration?  All packets should expire at the same time and cause the entire "stream" to expire

 - it feels like the endpoint itself should handle this responsibility with the help of some
   collaborator code

ITransportFiles (ITransportStreams?)
 - this is in contrast to ISplitMessagesIntoPackets which keeps the payload in memory
   whereas ITransportFiles doesn't attempt to keep everything in memory)
 - it still needs to be aware of expiration

ReplicatedSubscriptionStorage
 - Publisher notifies "central" subscription storage of intent to publish along with
   types of messages it will publish and expiration of publish (max publisher lifetime?).
   This occurs when the SubscriptionStorage is initialized, e.g. ctro

 - "Central" subscription storage receives publisher notification/annoucment and adds
   the annocument/life of messages publisher will publish to its list of publishers

 - Subscriber sends a sub/unsub request to the "publisher subscription endpoint" as normal.

 - When a sub/unsub request arrives, the central storage saves it to the DB and then iterates
   through the publishers and finds the publisher(s) that match the associated message types

 - central publisher subscription endpoint then forwards* the corresponding sub/unsub message
   to all publishers for the particular message type.
   * forwards = new sub/unsub message with just the associated types

 - the main advantage of this is that everything can be in memory for the publisher, e.g.
   when it goes to publish, it doesn't have to query a database.  It can simply go to its
   DiskBackedInMemorySubscriptionStorage instance.

 - The publisher would persist all sub/unsubs to disk but retain a copy in memory to avoid
   a disk read for every single invocation of "publish".

   OPTIONAL STUFF:
 - Heartbeat: Publisher renews "lifecycle lease" every X hours?
 - "Central" subscription storage stops forwarding Subscribe/Unsubscribe requests after X missed heartbeats
 - When publisher wakes up, central subscription store gets HB annoucment and forwards all corresponding
   state to publisher

 - this all could potentially be done using event sourcing with snapshots?